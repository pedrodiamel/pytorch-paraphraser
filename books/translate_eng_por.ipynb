{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)\n",
    "            \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('../rec/data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH \n",
    "        #and \\\n",
    "        #p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135671 sentence pairs\n",
      "Trimmed to 111526 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "por 16367\n",
      "eng 9940\n",
      "['vai .', 'go .']\n",
      "['va .', 'go .']\n",
      "['oi .', 'hi .']\n",
      "['corre !', 'run !']\n",
      "['corra !', 'run !']\n",
      "['corram !', 'run !']\n",
      "['corre !', 'run .']\n",
      "['corra !', 'run .']\n",
      "['corram !', 'run .']\n",
      "['quem ?', 'who ?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'por', True)\n",
    "# print(random.choice(pairs))\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed from 99839 pairs to 99839, 1.0000 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(input_lang, output_lang, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    input_lang.trim(MIN_COUNT)\n",
    "    output_lang.trim(MIN_COUNT)\n",
    "    \n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in input_lang.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in output_lang.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trimRareWords(input_lang, output_lang, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[20305, 23877, 19052, 20026, 20462],\n",
      "        [19891, 17843, 17210, 16410, 23582],\n",
      "        [23582, 23792, 21803, 23582, 21606],\n",
      "        [22247, 23800, 17763, 23042,     2],\n",
      "        [22525, 22148, 17735, 21931,     0],\n",
      "        [20693, 18136, 21606,     2,     0],\n",
      "        [23101, 20725,     2,     0,     0],\n",
      "        [18795, 21606,     0,     0,     0],\n",
      "        [21931,     2,     0,     0,     0],\n",
      "        [    2,     0,     0,     0,     0]])\n",
      "lengths: tensor([10,  9,  7,  6,  4])\n",
      "target_variable: tensor([[13072, 13670, 13670, 12853, 10188],\n",
      "        [14916, 10508, 12072, 14916, 14919],\n",
      "        [11226, 13159, 14739, 12817, 11735],\n",
      "        [11459, 10645, 14558, 11910,     2],\n",
      "        [12495, 10624, 11735, 13873,     0],\n",
      "        [15333, 11115,     2, 12150,     0],\n",
      "        [12676, 11156,     0,     2,     0],\n",
      "        [12150, 11735,     0,     0,     0],\n",
      "        [    2,     2,     0,     0,     0]])\n",
      "mask: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0]], dtype=torch.uint8)\n",
      "max_target_len: 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData( input_lang, output_lang , pair_batch):\n",
    "    \n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    \n",
    "    inp, lengths = inputVar(input_batch, input_lang)\n",
    "    output, mask, max_target_len = outputVar(output_batch, output_lang)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData( input_lang, output_lang , [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(\n",
    "    model_name, \n",
    "    input_lang, output_lang, \n",
    "    pairs, \n",
    "    encoder, \n",
    "    decoder, \n",
    "    encoder_optimizer, \n",
    "    decoder_optimizer, \n",
    "    embedding, \n",
    "    encoder_n_layers, \n",
    "    decoder_n_layers, \n",
    "    save_dir, \n",
    "    n_iteration, \n",
    "    batch_size, \n",
    "    print_every, \n",
    "    save_every, \n",
    "    clip, \n",
    "    corpus_name, \n",
    "    loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(input_lang, output_lang, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    plot_losses = []\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            plot_losses.append(print_loss_avg)\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'input_lang_dict': input_lang.__dict__,\n",
    "                'output_lang_dict': output_lang.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "                \n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "         \n",
    "    return plot_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, input_lang, output_lang, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(input_lang, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [output_lang.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, input_lang, output_lang):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, input_lang, output_lang, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "            \n",
    "            \n",
    "def evaluateRandomly(encoder, decoder, searcher, input_lang, output_lang, pair, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        input_sentence = normalizeString( pair[0] )\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, searcher, input_lang, output_lang, input_sentence )\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "corpus_name='anki'\n",
    "save_dir = os.path.join(\"../out\", \"save\")\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "# checkpoint_iter = 4000\n",
    "# loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    input_lang.__dict__ = checkpoint['input_lang_dict']\n",
    "    output_lang.__dict__ = checkpoint['output_lang_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding( input_lang.n_words, hidden_size)\n",
    "\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "    \n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, output_lang.n_words, decoder_n_layers, dropout)\n",
    "\n",
    "\n",
    "#input_lang, output_lang\n",
    "\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "    \n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(24280, 500)\n",
      "  (gru): GRU(500, 500, num_layers=2, dropout=0.1, bidirectional=True)\n",
      ")\n",
      "LuongAttnDecoderRNN(\n",
      "  (embedding): Embedding(24280, 500)\n",
      "  (embedding_dropout): Dropout(p=0.1)\n",
      "  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
      "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
      "  (out): Linear(in_features=500, out_features=15430, bias=True)\n",
      "  (attn): Attn()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 400; Percent complete: 4.0%; Average loss: 3.5709\n",
      "Iteration: 800; Percent complete: 8.0%; Average loss: 2.1563\n",
      "Iteration: 1200; Percent complete: 12.0%; Average loss: 1.5803\n",
      "Iteration: 1600; Percent complete: 16.0%; Average loss: 1.2395\n",
      "Iteration: 2000; Percent complete: 20.0%; Average loss: 1.0505\n",
      "Iteration: 2400; Percent complete: 24.0%; Average loss: 0.8948\n",
      "Iteration: 2800; Percent complete: 28.0%; Average loss: 0.7909\n",
      "Iteration: 3200; Percent complete: 32.0%; Average loss: 0.7088\n",
      "Iteration: 3600; Percent complete: 36.0%; Average loss: 0.6381\n",
      "Iteration: 4000; Percent complete: 40.0%; Average loss: 0.5871\n",
      "Iteration: 4400; Percent complete: 44.0%; Average loss: 0.5419\n",
      "Iteration: 4800; Percent complete: 48.0%; Average loss: 0.5113\n",
      "Iteration: 5200; Percent complete: 52.0%; Average loss: 0.4650\n",
      "Iteration: 5600; Percent complete: 56.0%; Average loss: 0.4383\n",
      "Iteration: 6000; Percent complete: 60.0%; Average loss: 0.4099\n",
      "Iteration: 6400; Percent complete: 64.0%; Average loss: 0.3876\n",
      "Iteration: 6800; Percent complete: 68.0%; Average loss: 0.3584\n",
      "Iteration: 7200; Percent complete: 72.0%; Average loss: 0.3461\n",
      "Iteration: 7600; Percent complete: 76.0%; Average loss: 0.3304\n",
      "Iteration: 8000; Percent complete: 80.0%; Average loss: 0.3170\n",
      "Iteration: 8400; Percent complete: 84.0%; Average loss: 0.2930\n",
      "Iteration: 8800; Percent complete: 88.0%; Average loss: 0.2848\n",
      "Iteration: 9200; Percent complete: 92.0%; Average loss: 0.2662\n",
      "Iteration: 9600; Percent complete: 96.0%; Average loss: 0.2552\n",
      "Iteration: 10000; Percent complete: 100.0%; Average loss: 0.2488\n"
     ]
    }
   ],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 10000\n",
    "print_every = 400\n",
    "save_every = 500\n",
    "\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "plot_losses = trainIters(model_name, input_lang, output_lang, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure( )\n",
    "    #fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    #loc = ticker.MultipleLocator(base=0.2)\n",
    "    #ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.title('Training error')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Error')\n",
    "    #plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "showPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> desligar wi-fi\n",
      "Bot: i feel constantly seriously .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c35efb2d6f07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Begin chatting (uncomment and run the following line to begin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# evaluateRandomly(encoder, decoder, searcher, input_lang, output_lang, pair )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-d2e165262f91>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, input_lang, output_lang)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Get input sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Check if it is quit case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, input_lang, output_lang)\n",
    "\n",
    "# evaluateRandomly(encoder, decoder, searcher, input_lang, output_lang, pair )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
