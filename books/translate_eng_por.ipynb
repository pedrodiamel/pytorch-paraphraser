{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "            \n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('../rec/data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH \n",
    "        #and \\\n",
    "        #p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 136328 sentence pairs\n",
      "Trimmed to 112183 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "por 16416\n",
      "eng 9972\n",
      "['vai .', 'go .']\n",
      "['va .', 'go .']\n",
      "['oi .', 'hi .']\n",
      "['corre !', 'run !']\n",
      "['corra !', 'run !']\n",
      "['corram !', 'run !']\n",
      "['corre !', 'run .']\n",
      "['corra !', 'run .']\n",
      "['corram !', 'run .']\n",
      "['quem ?', 'who ?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'por', True)\n",
    "# print(random.choice(pairs))\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 7957 / 16413 = 0.4848\n",
      "keep_words 5531 / 9969 = 0.5548\n",
      "Trimmed from 112183 pairs to 100474, 0.8956 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(input_lang, output_lang, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    input_lang.trim(MIN_COUNT)\n",
    "    output_lang.trim(MIN_COUNT)\n",
    "    \n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in input_lang.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in output_lang.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trimRareWords(input_lang, output_lang, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[ 16493,  16925,  16446,  16493,  19074],\n",
      "        [ 16502,  17908,  16849,  19441,  18672],\n",
      "        [ 16957,  17256,  16500,  16464,  16811],\n",
      "        [ 20126,  16844,  16493,  17253,  17167],\n",
      "        [ 16495,  16995,  16502,  17265,  21299],\n",
      "        [ 17420,  17708,  16530,  16417,  16417],\n",
      "        [ 16417,  16417,  16417,      2,      2],\n",
      "        [     2,      2,      2,      0,      0]])\n",
      "lengths: tensor([ 8,  8,  8,  7,  7])\n",
      "target_variable: tensor([[ 10020,  10458,   9987,  10649,  10962],\n",
      "        [ 10045,  10889,  10084,  11879,  11373],\n",
      "        [ 10362,  12256,  10085,  10404,  10349],\n",
      "        [ 11052,  10501,  10246,  10004,  10225],\n",
      "        [ 10589,   9973,  10020,  10199,  11233],\n",
      "        [  9973,      2,  10362,   9973,   9973],\n",
      "        [     2,      0,  10030,      2,      2],\n",
      "        [     0,      0,   9973,      0,      0],\n",
      "        [     0,      0,      2,      0,      0]])\n",
      "mask: tensor([[ 1,  1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  1,  1],\n",
      "        [ 1,  0,  1,  1,  1],\n",
      "        [ 0,  0,  1,  0,  0],\n",
      "        [ 0,  0,  1,  0,  0]], dtype=torch.uint8)\n",
      "max_target_len: 9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData( input_lang, output_lang , pair_batch):\n",
    "    \n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    \n",
    "    inp, lengths = inputVar(input_batch, input_lang)\n",
    "    output, mask, max_target_len = outputVar(output_batch, output_lang)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData( input_lang, output_lang , [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(\n",
    "    model_name, \n",
    "    input_lang, output_lang, \n",
    "    pairs, \n",
    "    encoder, \n",
    "    decoder, \n",
    "    encoder_optimizer, \n",
    "    decoder_optimizer, \n",
    "    embedding, \n",
    "    encoder_n_layers, \n",
    "    decoder_n_layers, \n",
    "    save_dir, \n",
    "    n_iteration, \n",
    "    batch_size, \n",
    "    print_every, \n",
    "    save_every, \n",
    "    clip, \n",
    "    corpus_name, \n",
    "    loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(input_lang, output_lang, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    plot_losses = []\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            plot_losses.append(print_loss_avg)\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'input_lang_dict': input_lang.__dict__,\n",
    "                'output_lang_dict': output_lang.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "                \n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "         \n",
    "    return plot_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, input_lang, output_lang, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(input_lang, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [output_lang.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, input_lang, output_lang):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, input_lang, output_lang, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")\n",
    "            \n",
    "            \n",
    "def evaluateRandomly(encoder, decoder, searcher, input_lang, output_lang, pair, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        input_sentence = normalizeString( pair[0] )\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = evaluate(encoder, decoder, searcher, input_lang, output_lang, input_sentence )\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "corpus_name='anki'\n",
    "save_dir = os.path.join(\"../out\", \"save\")\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 4000\n",
    "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "                           '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "                           '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    input_lang.__dict__ = checkpoint['input_lang_dict']\n",
    "    output_lang.__dict__ = checkpoint['output_lang_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding( input_lang.n_words, hidden_size)\n",
    "\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "    \n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, output_lang.n_words, decoder_n_layers, dropout)\n",
    "\n",
    "\n",
    "#input_lang, output_lang\n",
    "\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "    \n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(24373, 500)\n",
      "  (gru): GRU(500, 500, num_layers=2, dropout=0.1, bidirectional=True)\n",
      ")\n",
      "LuongAttnDecoderRNN(\n",
      "  (embedding): Embedding(24373, 500)\n",
      "  (embedding_dropout): Dropout(p=0.1)\n",
      "  (gru): GRU(500, 500, num_layers=2, dropout=0.1)\n",
      "  (concat): Linear(in_features=1000, out_features=500, bias=True)\n",
      "  (out): Linear(in_features=500, out_features=15503, bias=True)\n",
      "  (attn): Attn()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 100; Percent complete: 2.5%; Average loss: 3.6145\n",
      "Iteration: 200; Percent complete: 5.0%; Average loss: 3.0275\n",
      "Iteration: 300; Percent complete: 7.5%; Average loss: 2.6509\n",
      "Iteration: 400; Percent complete: 10.0%; Average loss: 2.3694\n",
      "Iteration: 500; Percent complete: 12.5%; Average loss: 2.1313\n",
      "Iteration: 600; Percent complete: 15.0%; Average loss: 1.9567\n",
      "Iteration: 700; Percent complete: 17.5%; Average loss: 1.8054\n",
      "Iteration: 800; Percent complete: 20.0%; Average loss: 1.6739\n",
      "Iteration: 900; Percent complete: 22.5%; Average loss: 1.5482\n",
      "Iteration: 1000; Percent complete: 25.0%; Average loss: 1.4453\n",
      "Iteration: 1100; Percent complete: 27.5%; Average loss: 1.3867\n",
      "Iteration: 1200; Percent complete: 30.0%; Average loss: 1.2729\n",
      "Iteration: 1300; Percent complete: 32.5%; Average loss: 1.2219\n",
      "Iteration: 1400; Percent complete: 35.0%; Average loss: 1.1650\n",
      "Iteration: 1500; Percent complete: 37.5%; Average loss: 1.1193\n",
      "Iteration: 1600; Percent complete: 40.0%; Average loss: 1.0483\n",
      "Iteration: 1700; Percent complete: 42.5%; Average loss: 1.0174\n",
      "Iteration: 1800; Percent complete: 45.0%; Average loss: 0.9797\n",
      "Iteration: 1900; Percent complete: 47.5%; Average loss: 0.9461\n",
      "Iteration: 2000; Percent complete: 50.0%; Average loss: 0.9100\n",
      "Iteration: 2100; Percent complete: 52.5%; Average loss: 0.8699\n",
      "Iteration: 2200; Percent complete: 55.0%; Average loss: 0.8411\n",
      "Iteration: 2300; Percent complete: 57.5%; Average loss: 0.8196\n",
      "Iteration: 2400; Percent complete: 60.0%; Average loss: 0.8088\n",
      "Iteration: 2500; Percent complete: 62.5%; Average loss: 0.7535\n",
      "Iteration: 2600; Percent complete: 65.0%; Average loss: 0.7580\n",
      "Iteration: 2700; Percent complete: 67.5%; Average loss: 0.7167\n",
      "Iteration: 2800; Percent complete: 70.0%; Average loss: 0.6971\n",
      "Iteration: 2900; Percent complete: 72.5%; Average loss: 0.6916\n",
      "Iteration: 3000; Percent complete: 75.0%; Average loss: 0.6705\n",
      "Iteration: 3100; Percent complete: 77.5%; Average loss: 0.6624\n",
      "Iteration: 3200; Percent complete: 80.0%; Average loss: 0.6328\n",
      "Iteration: 3300; Percent complete: 82.5%; Average loss: 0.6259\n",
      "Iteration: 3400; Percent complete: 85.0%; Average loss: 0.6180\n",
      "Iteration: 3500; Percent complete: 87.5%; Average loss: 0.5958\n",
      "Iteration: 3600; Percent complete: 90.0%; Average loss: 0.5798\n",
      "Iteration: 3700; Percent complete: 92.5%; Average loss: 0.5666\n",
      "Iteration: 3800; Percent complete: 95.0%; Average loss: 0.5611\n",
      "Iteration: 3900; Percent complete: 97.5%; Average loss: 0.5501\n",
      "Iteration: 4000; Percent complete: 100.0%; Average loss: 0.5324\n"
     ]
    }
   ],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 4000\n",
    "print_every = 100\n",
    "save_every = 500\n",
    "\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "plot_losses = trainIters(model_name, input_lang, output_lang, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure( )\n",
    "    #fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    #loc = ticker.MultipleLocator(base=0.2)\n",
    "    #ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.title('Training error')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Error')\n",
    "    #plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "showPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  ligar wi-fi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: turn wifi on .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  desligar wi-fi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: turn off wifi .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  ativar dados do celular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: turn on the cellphone off .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  use trs g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: use the three g .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  fechar trs g\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: close three g .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  ligue vibrao\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: turn on the vibration .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  desabilitar o modo avio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: disable the airplane mode .\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, input_lang, output_lang)\n",
    "\n",
    "# evaluateRandomly(encoder, decoder, searcher, input_lang, output_lang, pair )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
