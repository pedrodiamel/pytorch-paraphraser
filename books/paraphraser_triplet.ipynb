{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from six import iteritems\n",
    "\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, ','), (1, '.'), (2, 'the'), (3, 'and'), (4, 'to'), (5, 'of'), (6, 'a'), (7, 'in'), (8, ':'), (9, 'is')]\n",
      "74602\n",
      "(74666, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_sentence_embeddings_jw( pathname ):\n",
    "    '''Read John Wieting sentence embeddings'''\n",
    "    with open(pathname , 'rb') as f:\n",
    "        # [ numpy.ndarray(95283, 300), numpy.ndarray(74664, 300), (trigram_dict, word_dict)]\n",
    "        x = pickle.load(f, encoding='latin1')\n",
    "        word_vocab_size, embedding_size = x[1].shape\n",
    "        trigram_embeddings, word_embeddings, _ = x\n",
    "        \n",
    "        trigram_to_id, word_to_id = x[2]\n",
    "        word_to_id['<START>'] = word_vocab_size\n",
    "        word_to_id['<END>'] = word_vocab_size + 1\n",
    "        idx_to_word = { idx: word for word, idx in iteritems(word_to_id) }\n",
    "        word_embeddings = np.vstack((word_embeddings, np.random.randn(2, embedding_size)))\n",
    "\n",
    "        return ( word_to_id, \n",
    "                 idx_to_word, \n",
    "                 word_embeddings, \n",
    "                 word_to_id['<START>'], \n",
    "                 word_to_id['<END>'], \n",
    "                 word_to_id['UUUNKKK'], \n",
    "                 word_to_id['★']\n",
    "               )\n",
    "    \n",
    "\n",
    "pathname='../rec/data/ngram-word-concat-40.pickle'\n",
    "word_to_id, id_to_word, embeddings, start_id, end_id, unk_id, mask_id = read_sentence_embeddings_jw( pathname )\n",
    "\n",
    "print( [ (word_to_id[id_to_word[i]], id_to_word[i]) for i in range(10) ] )\n",
    "print( len(word_to_id) )\n",
    "print( embeddings.shape )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74602\n",
      "74602\n",
      "74602\n",
      "(74666, 300)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Dictionary:\n",
    "    def __init__(self ):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.n_words = 0\n",
    "        self.embeddings = []\n",
    "        \n",
    "        self.PAD_token = 0  # Used for padding short sentences\n",
    "        self.SOS_token = 1  # Start-of-sentence token\n",
    "        self.EOS_token = 2  # End-of-sentence token\n",
    "        self.UNK_token = 3 \n",
    "\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def load_embeddings_jw( self, pathname ):\n",
    "        '''Read John Wieting sentence embeddings'''\n",
    "        with open(pathname , 'rb') as f:\n",
    "            # [ numpy.ndarray(95283, 300), numpy.ndarray(74664, 300), (trigram_dict, word_dict)]\n",
    "            x = pickle.load(f, encoding='latin1')\n",
    "            \n",
    "            word_vocab_size, embedding_size = x[1].shape\n",
    "            trigram_embeddings, word_embeddings, _ = x\n",
    "\n",
    "            trigram_to_id, word_to_id = x[2]\n",
    "            \n",
    "            word_to_id['<START>'] = word_vocab_size\n",
    "            word_to_id['<END>']   = word_vocab_size + 1\n",
    "                                   \n",
    "            idx_to_word = { idx: word for word, idx in iteritems(word_to_id) }\n",
    "            word_embeddings = np.vstack((word_embeddings, np.random.randn(2, embedding_size)))\n",
    "            word_to_count = { word: 1 for word, idx in iteritems(word_to_id) }\n",
    "            \n",
    "            self.word2index = word_to_id\n",
    "            self.index2word = idx_to_word\n",
    "            self.word2count = word_to_count\n",
    "            self.embeddings = word_embeddings\n",
    "            self.n_words    = len(word_to_id)\n",
    "            \n",
    "            self.PAD_token = word_to_id['★']        # Used for padding short sentences\n",
    "            self.SOS_token = word_to_id['<START>']  # Start-of-sentence token\n",
    "            self.EOS_token = word_to_id['<END>']    # End-of-sentence token\n",
    "            self.UNK_token = word_to_id['UUUNKKK'] \n",
    "\n",
    "\n",
    "pathname='../rec/data/ngram-word-concat-40.pickle'\n",
    "dictionaty = Dictionary()\n",
    "dictionaty.load_embeddings_jw( pathname )\n",
    "print( len(dictionaty.word2index) )\n",
    "print( len(dictionaty.index2word) )\n",
    "print( len(dictionaty.word2count) )\n",
    "print( dictionaty.embeddings.shape )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "['of course you did .', 'of course it is .']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def openmp_nlp_pipeline(lines, n_threads=12):\n",
    "    ''' Execute spacy's openmp nlp pipeline '''\n",
    "    return [ [ token.lower_ for token in doc ] for doc in nlp.pipe(lines, n_threads=n_threads, disable=['parser', 'tagger', 'ner']) ]\n",
    "\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def read_paraphraser( pathname ):\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open(pathname, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]    # normalizeString, openmp_nlp_pipeline,\n",
    "        \n",
    "#     source_sentences = []\n",
    "#     ref_sentences = []    \n",
    "#     with open(pathname, 'r') as f:\n",
    "#         for i, line in enumerate(f):\n",
    "#             source, ref = line.split('\\t')\n",
    "#             source_sentences.append(source.strip())\n",
    "#             ref_sentences.append(ref.strip())    \n",
    "#     source_sentences = openmp_nlp_pipeline( source_sentences )\n",
    "#     ref_sentences = openmp_nlp_pipeline( ref_sentences )\n",
    "#     pairs = np.stack( ( source_sentences, ref_sentences), axis=1 )    \n",
    "    \n",
    "    return pairs\n",
    "\n",
    "\n",
    "pathname = '../rec/data/para-nmt-50m-small.txt'\n",
    "pairs = read_paraphraser( pathname )\n",
    "print( pairs[1] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH \n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 99968 sentence pairs\n",
      "Counted words:\n",
      "74602\n",
      "44299\n",
      "['of course you did .', 'of course it is .']\n",
      "[' why not ?', ' why not ?']\n",
      "['an old man s mistake ', 'an old man s fault . . . ']\n",
      "['he loved that little man by the way .', 'he liked the little boy .']\n",
      "['provide the following information', 'enter the following information']\n",
      "['i ve been very very lucky .', 'i was just lucky .']\n",
      "['why are you carrying around grass ?', 'why did you pull the grass ?']\n",
      "['not as much as i d like', 'not as often as i d like .']\n",
      "['i do n t like that crap .', 'i do n t like it .']\n",
      "['from a marine in da nang ', 'from the sailor in da nang .']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_data( pathname, pathdictionary ):\n",
    "    pairs = read_paraphraser( pathname)\n",
    "    dictionary = Dictionary()\n",
    "    dictionary.load_embeddings_jw( pathdictionary )\n",
    "    \n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "        \n",
    "    print(\"Counted words:\")\n",
    "    print(dictionary.n_words)\n",
    "    return dictionary, pairs\n",
    "\n",
    "\n",
    "pathname = '../rec/data/para-nmt-50m-small.txt'\n",
    "pathdictionary = '../rec/data/ngram-word-concat-40.pickle'\n",
    "\n",
    "dictionary, pairs = prepare_data( pathname, pathdictionary )\n",
    "\n",
    "# print(random.choice(pairs))\n",
    "print(len(pairs))\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1_variable: tensor([[     2,     17,     49,  74663,      2],\n",
      "        [  8696,    103,     28,  18765,  14788],\n",
      "        [    18,     42,     17,   7455,    205],\n",
      "        [     2,   3345,      1,      1,   6801],\n",
      "        [  1139,   5306,  74665,  74665,  74665],\n",
      "        [    35,     35,  74665,  74665,  74665],\n",
      "        [ 74665,  74665,  74665,  74665,  74665]])\n",
      "lengths: 7\n",
      "mask: tensor([[ 1,  1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  1,  1],\n",
      "        [ 1,  1,  0,  0,  0],\n",
      "        [ 1,  1,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0]], dtype=torch.uint8)\n",
      "s2_variable: tensor([[ 64406,     45,      3,  74663,   3121],\n",
      "        [    18,     17,     49,   7297,  14788],\n",
      "        [     2,    103,      5,    234,  74665],\n",
      "        [  1139,     42,    164,  18765,  74665],\n",
      "        [    35,    743,    703,   3710,  74665],\n",
      "        [ 74665,   5306,     28,      1,  74665],\n",
      "        [ 74665,     35,     17,  74665,  74665],\n",
      "        [ 74665,  74665,      1,  74665,  74665],\n",
      "        [ 74665,  74665,  74665,  74665,  74665]])\n",
      "lengths: 9\n",
      "mask: tensor([[ 1,  1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  1,  0],\n",
      "        [ 1,  1,  1,  1,  0],\n",
      "        [ 1,  1,  1,  1,  0],\n",
      "        [ 0,  1,  1,  1,  0],\n",
      "        [ 0,  1,  1,  0,  0],\n",
      "        [ 0,  0,  1,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0]], dtype=torch.uint8)\n",
      "t1_variable: tensor([[     3,  64406,     45,   3121,  74663],\n",
      "        [    49,     18,     17,  14788,   7297],\n",
      "        [     5,      2,    103,  74665,    234],\n",
      "        [   164,   1139,     42,  74665,  18765],\n",
      "        [   703,     35,    743,  74665,   3710],\n",
      "        [    28,  74665,   5306,  74665,      1],\n",
      "        [    17,  74665,     35,  74665,  74665],\n",
      "        [     1,  74665,  74665,  74665,  74665],\n",
      "        [ 74665,  74665,  74665,  74665,  74665]])\n",
      "lengths: 9\n",
      "mask: tensor([[ 1,  1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  0,  1],\n",
      "        [ 1,  1,  1,  0,  1],\n",
      "        [ 1,  1,  1,  0,  1],\n",
      "        [ 1,  0,  1,  0,  1],\n",
      "        [ 1,  0,  1,  0,  0],\n",
      "        [ 1,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "\n",
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index.get(word, voc.UNK_token )  for word in sentence.split(' ')] + [ voc.EOS_token ]\n",
    "\n",
    "def zeroPadding(l, fillvalue):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == value:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch, voc.EOS_token)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch, voc.EOS_token)\n",
    "    mask = binaryMatrix(padList, voc.EOS_token)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "\n",
    "def get_triplets( pairs ):\n",
    "    n = len(pairs)\n",
    "    i = np.arange( n )\n",
    "    j = np.arange( n )\n",
    "    while np.sum( (np.abs(i-j) == 0 ) ) != 0:\n",
    "        random.shuffle( j )\n",
    "    #pairs = [ pairs[x] for x in j ]  \n",
    "    #p = [ x[ random.randint( 0,1 ) ] for x in pairs  ]    \n",
    "    triplets = [ ((pairs[i][0], pairs[i][1], pairs[j[i]][ random.randint( 0,1 ) ]))  for i in range(n) ]    \n",
    "    return triplets\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData( dictionary , pair_batch):\n",
    "    \n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    triple_batch = get_triplets(pair_batch)\n",
    "    s1_batch, s2_batch, t1_batch = [], [], []\n",
    "    for triple in triple_batch :\n",
    "        s1_batch.append(triple[0])\n",
    "        s2_batch.append(triple[1])\n",
    "        t1_batch.append(triple[2])\n",
    "    #inp, lengths = inputVar(input_batch, dictionary)\n",
    "    #output, mask, max_target_len = outputVar(output_batch, dictionary)    \n",
    "    s1, s1_mask, s1_max_len = outputVar(s1_batch, dictionary)\n",
    "    s2, s2_mask, s2_max_len = outputVar(s2_batch, dictionary)\n",
    "    t1, t1_mask, t1_max_len = outputVar(t1_batch, dictionary)    \n",
    "    return s1, s1_mask, s1_max_len, s2, s2_mask, s2_max_len, t1, t1_mask, t1_max_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData( dictionary, [random.choice(pairs) for _ in range(small_batch_size)] )\n",
    "s1, s1_mask, s1_max_len, s2, s2_mask, s2_max_len, t1, t1_mask, t1_max_len = batches\n",
    "\n",
    "print(\"s1_variable:\", s1)\n",
    "print(\"lengths:\", s1_max_len)\n",
    "print(\"mask:\", s1_mask)\n",
    "\n",
    "print(\"s2_variable:\", s2)\n",
    "print(\"lengths:\", s2_max_len)\n",
    "print(\"mask:\", s2_mask)\n",
    "\n",
    "print(\"t1_variable:\", t1)\n",
    "print(\"lengths:\", t1_max_len)\n",
    "print(\"mask:\", t1_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5])\n",
      "torch.Size([7, 5])\n",
      "torch.Size([5, 300])\n",
      "torch.Size([5, 300])\n",
      "torch.Size([5, 300])\n"
     ]
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden\n",
    "\n",
    "    \n",
    "class EncoderAvg(nn.Module):\n",
    "    def __init__(self, embedding, tonorm=True):\n",
    "        super(EncoderAvg, self).__init__()\n",
    "        self.embedding = embedding      \n",
    "        self.tonorm = tonorm\n",
    "\n",
    "    def forward(self, input_seq, input_mask ):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)        \n",
    "        #Average\n",
    "        outputs = (embedded * input_mask.view( input_mask.shape[0],input_mask.shape[1],-1 ).float())  \n",
    "        outputs = outputs.sum( dim=0 ) \n",
    "        if self.tonorm: \n",
    "            outputs = outputs / input_mask.float().sum(dim=0).unsqueeze(dim=1)        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class Tripletnet(nn.Module):\n",
    "    def __init__(self, embeddingnet):\n",
    "        super(Tripletnet, self).__init__()\n",
    "        self.embeddingnet = embeddingnet\n",
    "    \n",
    "    def forward(self, s1, s1_mask, s2, s2_mask, t1, t1_mask):\n",
    "        embedded_s1 = self.embeddingnet(s1, s1_mask)\n",
    "        embedded_s2 = self.embeddingnet(s2, s2_mask)\n",
    "        embedded_t1 = self.embeddingnet(t1, t1_mask)\n",
    "        return embedded_s1, embedded_s2, embedded_t1\n",
    "    \n",
    "    \n",
    "embedding = nn.Embedding.from_pretrained( torch.from_numpy( dictionary.embeddings ).float() )\n",
    "encmodel = EncoderAvg( embedding )\n",
    "tripmodel = Tripletnet( encmodel )\n",
    "\n",
    "\n",
    "print( s1.shape )\n",
    "print( s1_mask.shape )\n",
    "\n",
    "# s1_ave_enc = encmodel(  s1, s1_mask )\n",
    "# s2_ave_enc = encmodel(  s2, s2_mask )\n",
    "# t1_ave_enc = encmodel(  t1, t1_mask )\n",
    "\n",
    "s1_ave_enc, s2_ave_enc, t1_ave_enc = tripmodel( s1, s1_mask, s2, s2_mask, t1, t1_mask )\n",
    "\n",
    "print( s1_ave_enc.shape )\n",
    "print( s2_ave_enc.shape )\n",
    "print( t1_ave_enc.shape )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()\n",
    "\n",
    "def tripletCosineLoss(anchor, positive, negative, margin=1.0, eps=1e-6):        \n",
    "    target = torch.ones( anchor.shape[0] ).to(device)\n",
    "    coss1s2 = F.cosine_similarity(anchor, positive, dim=1, eps=eps)\n",
    "    coss1t1 = F.cosine_similarity(anchor, negative, dim=1, eps=eps)    \n",
    "    #triplet_loss = nn.MarginRankingLoss(margin=margin)( coss1s2, coss1t1, target)\n",
    "    triplet_loss = F.relu( margin - coss1s2 + coss1t1  ).mean()\n",
    "    return triplet_loss\n",
    "\n",
    "# loss = tripletCosineLoss(s1_ave_enc, s2_ave_enc, t1_ave_enc)\n",
    "# print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(\n",
    "    s1, s1_mask, s1_max_len, \n",
    "    s2, s2_mask, s2_max_len, \n",
    "    t1, t1_mask, t1_max_len,\n",
    "    model, \n",
    "    embedding,\n",
    "    model_optimizer, \n",
    "    batch_size, \n",
    "    clip\n",
    "    ):\n",
    "\n",
    "    # Zero gradients\n",
    "    model_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    s1 = s1.to(device)\n",
    "    s1_mask = s1_mask.to(device)    \n",
    "    s2 = s2.to(device)\n",
    "    s2_mask = s2_mask.to(device)    \n",
    "    t1 = t1.to(device)\n",
    "    t1_mask = t1_mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = 0\n",
    "    \n",
    "    # Forward pass through encoder\n",
    "    emb_s1, emb_s2, emb_t1 = model(s1, s1_mask, s2, s2_mask, t1, t1_mask)    \n",
    "    triplet_loss = tripletCosineLoss( emb_s1, emb_s2, emb_t1 )\n",
    "    loss = triplet_loss\n",
    "    print_losses = triplet_loss.item()\n",
    "    \n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    #_ = torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    \n",
    "    # Adjust model weights\n",
    "    model_optimizer.step()\n",
    "\n",
    "    return print_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(\n",
    "    model_name, \n",
    "    dictionary, \n",
    "    pairs, \n",
    "    model, \n",
    "    model_optimizer, \n",
    "    embedding, \n",
    "    save_dir, \n",
    "    n_iteration, \n",
    "    batch_size, \n",
    "    print_every, \n",
    "    save_every, \n",
    "    clip, \n",
    "    corpus_name, \n",
    "    loadFilename\n",
    "    ):\n",
    "        \n",
    "    # Load batches for each iteration\n",
    "    #training_batches = [batch2TrainData(dictionary, [random.choice(pairs) for _ in range(batch_size)])\n",
    "    #                  for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    plot_losses = []\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        #training_batch = training_batches[iteration - 1]\n",
    "        training_batch = batch2TrainData(dictionary, [random.choice(pairs) for _ in range(batch_size)])\n",
    "        \n",
    "        # Extract fields from batch\n",
    "        s1, s1_mask, s1_max_len, s2, s2_mask, s2_max_len, t1, t1_mask, t1_max_len = training_batch\n",
    "        \n",
    "        # Run a training iteration with batch\n",
    "        loss = train(\n",
    "            s1, s1_mask, s1_max_len, \n",
    "            s2, s2_mask, s2_max_len, \n",
    "            t1, t1_mask, t1_max_len, \n",
    "            model, \n",
    "            embedding, \n",
    "            model_optimizer, \n",
    "            batch_size, \n",
    "            clip\n",
    "            )\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            plot_losses.append(print_loss_avg)\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            #directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name )\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'model': model.state_dict(),\n",
    "                'model_opt': model_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'dictionary': dictionary.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "                \n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "         \n",
    "    return plot_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> his voice rose .\n",
      "> he raised his voice .\n",
      "> ai n t that what you said ?\n",
      "< 0.848|-0.046\n",
      "\n",
      "> he went across the street to the casino .\n",
      "> he went across to the casino .\n",
      "> a little zim zam zoom some chop socky .\n",
      "< 0.822|-0.073\n",
      "\n",
      "> you re a genius .\n",
      "> you re a genius are n t you ?\n",
      "> you il never catch her pittsburgh !\n",
      "< 0.519|0.005\n",
      "\n",
      "> red i am sorry . . .\n",
      "> rudoch i m sorry .\n",
      "> i ll uh . . .\n",
      "< 0.790|0.299\n",
      "\n",
      "> jesus gunney !\n",
      "> jesus sarge !\n",
      "> i love it .\n",
      "< 0.714|-0.156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(encmodel, dictionary, sentence, max_length=MAX_LENGTH):    \n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(dictionary, sentence)]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch, dictionary.EOS_token)\n",
    "    mask = binaryMatrix(padList, dictionary.EOS_token)\n",
    "    \n",
    "    mask = torch.ByteTensor(mask)\n",
    "    input_batch = torch.LongTensor(padList) #.transpose(0, 1)        \n",
    "    input_batch = input_batch.to(device)\n",
    "    mask = mask.to(device)\n",
    "    \n",
    "    input_enc = encmodel( input_batch,  mask )        \n",
    "    return input_enc\n",
    "\n",
    "\n",
    "\n",
    "def evaluateRandomly(encmodel, dictionary, pair, n=5):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        input_sentence_s1 = normalizeString( pair[0] )\n",
    "        input_sentence_s2 = normalizeString( pair[1] )\n",
    "        pair = random.choice(pairs)\n",
    "        input_sentence_t1 = normalizeString( pair[0] )\n",
    "                \n",
    "        print('>', input_sentence_s1)\n",
    "        print('>', input_sentence_s2)\n",
    "        print('>', input_sentence_t1)        \n",
    "        \n",
    "        input_sentence_s1_enc = evaluate(encmodel, dictionary, input_sentence_s1 )\n",
    "        input_sentence_s2_enc = evaluate(encmodel, dictionary, input_sentence_s2 )\n",
    "        input_sentence_t1_enc = evaluate(encmodel, dictionary, input_sentence_t1 )\n",
    "        \n",
    "        eps=1e-6\n",
    "        dist_s1s2 = F.cosine_similarity(input_sentence_s1_enc, input_sentence_s2_enc, dim=1, eps=eps)\n",
    "        dist_s1t1 = F.cosine_similarity(input_sentence_s1_enc, input_sentence_t1_enc, dim=1, eps=eps)\n",
    "                \n",
    "        output = '{:.3f}|{:.3f}'.format(dist_s1s2.item(), dist_s1t1.item())\n",
    "        \n",
    "        print('<', output)\n",
    "        print('')\n",
    "\n",
    "        \n",
    "# # ['of course you did .', 'of course it is .']\n",
    "# # [' why not ?', ' why not ?']\n",
    "# input_sentence = 'of course you did .'\n",
    "# input_sentence = normalizeString(input_sentence)\n",
    "# print(input_sentence)\n",
    "\n",
    "# input_enc = evaluate(encoder, dictionary, input_sentence)\n",
    "# print(input_enc.shape)\n",
    "\n",
    "evaluateRandomly(encoder, dictionary, pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Configure models\n",
    "model_name = 'triplet_model'\n",
    "\n",
    "#enc_model = 'wordaverage'\n",
    "#enc_model = 'maxpool'\n",
    "#enc_model = 'lstmavg'\n",
    "#enc_model = 'bilstmavg'\n",
    "#enc_model = ''\n",
    "\n",
    "\n",
    "#dropout = 0.1\n",
    "batch_size = 64 #64\n",
    "corpus_name='para-nmt-50m-small'\n",
    "save_dir = os.path.join(\"../out\", \"save\")\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "# checkpoint_iter = 4000\n",
    "# loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    model_sd = checkpoint['model']\n",
    "    model_optimizer_sd = checkpoint['model_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    dictionary.__dict__ = checkpoint['dictionary']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "# embedding = nn.Embedding( 74666, 300 ) #dictionary.n_words\n",
    "embedding = nn.Embedding.from_pretrained( torch.from_numpy( dictionary.embeddings ).float(),  freeze=False )\n",
    "\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "    \n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderAvg( embedding )\n",
    "model = Tripletnet( encoder )\n",
    "\n",
    "#input_lang, output_lang\n",
    "\n",
    "if loadFilename:\n",
    "    model.load_state_dict(model_sd)\n",
    "    \n",
    "# Use appropriate device\n",
    "model = model.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(74666, 300)\n",
      "EncoderAvg(\n",
      "  (embedding): Embedding(74666, 300)\n",
      ")\n",
      "Tripletnet(\n",
      "  (embeddingnet): EncoderAvg(\n",
      "    (embedding): Embedding(74666, 300)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(embedding)\n",
    "print(encoder)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 10; Percent complete: 1.0%; Average loss: 0.1883\n",
      "Iteration: 20; Percent complete: 2.0%; Average loss: 0.1949\n",
      "Iteration: 30; Percent complete: 3.0%; Average loss: 0.1981\n",
      "Iteration: 40; Percent complete: 4.0%; Average loss: 0.1819\n",
      "Iteration: 50; Percent complete: 5.0%; Average loss: 0.1976\n",
      "Iteration: 60; Percent complete: 6.0%; Average loss: 0.1813\n",
      "Iteration: 70; Percent complete: 7.0%; Average loss: 0.1893\n",
      "Iteration: 80; Percent complete: 8.0%; Average loss: 0.1845\n",
      "Iteration: 90; Percent complete: 9.0%; Average loss: 0.1868\n",
      "Iteration: 100; Percent complete: 10.0%; Average loss: 0.1936\n",
      "Iteration: 110; Percent complete: 11.0%; Average loss: 0.1779\n",
      "Iteration: 120; Percent complete: 12.0%; Average loss: 0.1898\n",
      "Iteration: 130; Percent complete: 13.0%; Average loss: 0.1930\n",
      "Iteration: 140; Percent complete: 14.0%; Average loss: 0.1980\n",
      "Iteration: 150; Percent complete: 15.0%; Average loss: 0.1863\n",
      "Iteration: 160; Percent complete: 16.0%; Average loss: 0.1945\n",
      "Iteration: 170; Percent complete: 17.0%; Average loss: 0.1950\n",
      "Iteration: 180; Percent complete: 18.0%; Average loss: 0.1964\n",
      "Iteration: 190; Percent complete: 19.0%; Average loss: 0.1860\n",
      "Iteration: 200; Percent complete: 20.0%; Average loss: 0.1790\n",
      "Iteration: 210; Percent complete: 21.0%; Average loss: 0.1846\n",
      "Iteration: 220; Percent complete: 22.0%; Average loss: 0.1817\n",
      "Iteration: 230; Percent complete: 23.0%; Average loss: 0.1861\n",
      "Iteration: 240; Percent complete: 24.0%; Average loss: 0.1760\n",
      "Iteration: 250; Percent complete: 25.0%; Average loss: 0.1920\n",
      "Iteration: 260; Percent complete: 26.0%; Average loss: 0.2002\n",
      "Iteration: 270; Percent complete: 27.0%; Average loss: 0.1976\n",
      "Iteration: 280; Percent complete: 28.0%; Average loss: 0.1958\n",
      "Iteration: 290; Percent complete: 29.0%; Average loss: 0.1938\n",
      "Iteration: 300; Percent complete: 30.0%; Average loss: 0.1946\n",
      "Iteration: 310; Percent complete: 31.0%; Average loss: 0.1756\n",
      "Iteration: 320; Percent complete: 32.0%; Average loss: 0.1987\n",
      "Iteration: 330; Percent complete: 33.0%; Average loss: 0.1885\n",
      "Iteration: 340; Percent complete: 34.0%; Average loss: 0.1882\n",
      "Iteration: 350; Percent complete: 35.0%; Average loss: 0.1863\n",
      "Iteration: 360; Percent complete: 36.0%; Average loss: 0.1848\n",
      "Iteration: 370; Percent complete: 37.0%; Average loss: 0.1729\n",
      "Iteration: 380; Percent complete: 38.0%; Average loss: 0.1810\n",
      "Iteration: 390; Percent complete: 39.0%; Average loss: 0.1938\n",
      "Iteration: 400; Percent complete: 40.0%; Average loss: 0.1790\n",
      "Iteration: 410; Percent complete: 41.0%; Average loss: 0.1955\n",
      "Iteration: 420; Percent complete: 42.0%; Average loss: 0.1938\n",
      "Iteration: 430; Percent complete: 43.0%; Average loss: 0.1949\n",
      "Iteration: 440; Percent complete: 44.0%; Average loss: 0.1828\n",
      "Iteration: 450; Percent complete: 45.0%; Average loss: 0.1872\n",
      "Iteration: 460; Percent complete: 46.0%; Average loss: 0.1860\n",
      "Iteration: 470; Percent complete: 47.0%; Average loss: 0.1718\n",
      "Iteration: 480; Percent complete: 48.0%; Average loss: 0.1908\n",
      "Iteration: 490; Percent complete: 49.0%; Average loss: 0.1995\n",
      "Iteration: 500; Percent complete: 50.0%; Average loss: 0.1841\n",
      "Iteration: 510; Percent complete: 51.0%; Average loss: 0.1844\n",
      "Iteration: 520; Percent complete: 52.0%; Average loss: 0.1852\n",
      "Iteration: 530; Percent complete: 53.0%; Average loss: 0.1850\n",
      "Iteration: 540; Percent complete: 54.0%; Average loss: 0.1734\n",
      "Iteration: 550; Percent complete: 55.0%; Average loss: 0.1873\n",
      "Iteration: 560; Percent complete: 56.0%; Average loss: 0.1855\n",
      "Iteration: 570; Percent complete: 57.0%; Average loss: 0.1621\n",
      "Iteration: 580; Percent complete: 58.0%; Average loss: 0.1844\n",
      "Iteration: 590; Percent complete: 59.0%; Average loss: 0.1925\n",
      "Iteration: 600; Percent complete: 60.0%; Average loss: 0.1928\n",
      "Iteration: 610; Percent complete: 61.0%; Average loss: 0.1885\n",
      "Iteration: 620; Percent complete: 62.0%; Average loss: 0.1851\n",
      "Iteration: 630; Percent complete: 63.0%; Average loss: 0.1873\n",
      "Iteration: 640; Percent complete: 64.0%; Average loss: 0.1867\n",
      "Iteration: 650; Percent complete: 65.0%; Average loss: 0.1856\n",
      "Iteration: 660; Percent complete: 66.0%; Average loss: 0.1914\n",
      "Iteration: 670; Percent complete: 67.0%; Average loss: 0.1866\n",
      "Iteration: 680; Percent complete: 68.0%; Average loss: 0.1793\n",
      "Iteration: 690; Percent complete: 69.0%; Average loss: 0.1819\n",
      "Iteration: 700; Percent complete: 70.0%; Average loss: 0.1718\n",
      "Iteration: 710; Percent complete: 71.0%; Average loss: 0.1812\n",
      "Iteration: 720; Percent complete: 72.0%; Average loss: 0.1854\n",
      "Iteration: 730; Percent complete: 73.0%; Average loss: 0.1784\n",
      "Iteration: 740; Percent complete: 74.0%; Average loss: 0.1859\n",
      "Iteration: 750; Percent complete: 75.0%; Average loss: 0.1823\n",
      "Iteration: 760; Percent complete: 76.0%; Average loss: 0.1927\n",
      "Iteration: 770; Percent complete: 77.0%; Average loss: 0.1697\n",
      "Iteration: 780; Percent complete: 78.0%; Average loss: 0.1898\n",
      "Iteration: 790; Percent complete: 79.0%; Average loss: 0.1805\n",
      "Iteration: 800; Percent complete: 80.0%; Average loss: 0.1769\n",
      "Iteration: 810; Percent complete: 81.0%; Average loss: 0.2071\n",
      "Iteration: 820; Percent complete: 82.0%; Average loss: 0.1793\n",
      "Iteration: 830; Percent complete: 83.0%; Average loss: 0.1788\n",
      "Iteration: 840; Percent complete: 84.0%; Average loss: 0.1731\n",
      "Iteration: 850; Percent complete: 85.0%; Average loss: 0.1852\n",
      "Iteration: 860; Percent complete: 86.0%; Average loss: 0.1785\n",
      "Iteration: 870; Percent complete: 87.0%; Average loss: 0.1730\n",
      "Iteration: 880; Percent complete: 88.0%; Average loss: 0.1802\n",
      "Iteration: 890; Percent complete: 89.0%; Average loss: 0.1662\n",
      "Iteration: 900; Percent complete: 90.0%; Average loss: 0.1784\n",
      "Iteration: 910; Percent complete: 91.0%; Average loss: 0.1793\n",
      "Iteration: 920; Percent complete: 92.0%; Average loss: 0.1807\n",
      "Iteration: 930; Percent complete: 93.0%; Average loss: 0.1830\n",
      "Iteration: 940; Percent complete: 94.0%; Average loss: 0.1834\n",
      "Iteration: 950; Percent complete: 95.0%; Average loss: 0.1692\n",
      "Iteration: 960; Percent complete: 96.0%; Average loss: 0.1783\n",
      "Iteration: 970; Percent complete: 97.0%; Average loss: 0.1881\n",
      "Iteration: 980; Percent complete: 98.0%; Average loss: 0.1886\n",
      "Iteration: 990; Percent complete: 99.0%; Average loss: 0.1913\n",
      "Iteration: 1000; Percent complete: 100.0%; Average loss: 0.1913\n"
     ]
    }
   ],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.001\n",
    "n_iteration = 1000\n",
    "print_every = 10\n",
    "save_every = 500\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "model.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "model_optimizer = optim.Adam( model.parameters() , lr=learning_rate ) #, amsgrad=True \n",
    "# model_optimizer = optim.SGD( model.parameters(), lr=learning_rate )\n",
    "\n",
    "if loadFilename:\n",
    "    model_optimizer.load_state_dict(model_optimizer_sd)\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "plot_losses = trainIters(\n",
    "    model_name, \n",
    "    dictionary, \n",
    "    pairs, \n",
    "    model,  \n",
    "    model_optimizer,\n",
    "    embedding, \n",
    "    save_dir, \n",
    "    n_iteration, \n",
    "    batch_size,\n",
    "    print_every, \n",
    "    save_every, \n",
    "    clip, \n",
    "    corpus_name, \n",
    "    loadFilename\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff93a7a1978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure( )\n",
    "    #fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    #loc = ticker.MultipleLocator(base=0.2)\n",
    "    #ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.title('Training error')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Error')\n",
    "    #plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
